{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "xXlWU-k1_LVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AIM** : Tokenize the sentence into words"
      ],
      "metadata": {
        "id": "2RFonXuA9oV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools** : Jupyter / any editor of python"
      ],
      "metadata": {
        "id": "q4Okh3s9De-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Library** : NLTK"
      ],
      "metadata": {
        "id": "E8yZW--FDrid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method** : Already available functions like\n",
        "\n",
        "Word_tokenize, Sent_tokenize, TreebankWordTokenizer, Wordpunct_tokenize, TweetTokenizer, MWETokenizer\n"
      ],
      "metadata": {
        "id": "aIkEZhVxDzv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (word_tokenize, sent_tokenize, TreebankWordTokenizer, wordpunct_tokenize, TweetTokenizer, MWETokenizer)\n",
        "text = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\""
      ],
      "metadata": {
        "id": "5YTJyhAzAplG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenizer**\n",
        "\n",
        "Word tokenizers are one class of tokenizers that split a text into words. These tokenizers can be used to create a bag of words representation of the text, which can be used for downstream tasks like building word2vec or TF-IDF models."
      ],
      "metadata": {
        "id": "Ikclw5IPJQPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "print(nltk_tokens)"
      ],
      "metadata": {
        "id": "M6DDLZGBJV72",
        "outputId": "0ec20fb4-5ae3-4ff1-ca7d-d8194df73acb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Tokenizer**\n",
        "\n",
        "Sentence tokenization is the process of splitting text into individual sentences. For literature, journalism, and formal documents the tokenization algorithms built in to spaCy perform well, since the tokenizer is trained on a corpus of formal English text. The sentence tokenizer performs less well for electronic health records featuring abbreviations, medical terms, spatial measurements, and other forms not found in standard written English."
      ],
      "metadata": {
        "id": "O11sNZlZOa6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens = nltk.sent_tokenize(text)\n",
        "print(nltk_tokens)"
      ],
      "metadata": {
        "id": "ogycIQgNOT9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca431ad-2e48-4053-b07e-af00774ef1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punctuation Based Tokenizer**\n",
        "\n",
        "This tokenizer splits the sentences into words based on whitespaces and punctuations."
      ],
      "metadata": {
        "id": "RpjDBN56Ommj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What you don't want to be done to yourself, don't do it to others.\"\n",
        "\n",
        "nltk_tokens = nltk.wordpunct_tokenize(text)\n",
        "print(nltk_tokens)"
      ],
      "metadata": {
        "id": "NSZyO_riOu7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3330b6-1dec-4741-e88d-809fa34c5805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'you', 'don', \"'\", 't', 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'don', \"'\", 't', 'do', 'it', 'to', 'others', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treebank Word Tokenizer**\n",
        "\n",
        "The problem which we had in the punctuation tokenizer of splitting the words into an incorrect format like doesnâ€™t into doesn, â€˜, and t but now the problem is solved. Treebank tokenizer contains rules for English contractions."
      ],
      "metadata": {
        "id": "yOsNBUs0O2yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What you don't want to be done to yourself, don't do it to others.\"\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "J1RcyapxO8BC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535c70d7-e724-413a-df41-d614c8b4c712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'it', 'to', 'others', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tweet Tokenizer**\n",
        "\n",
        "TweetTokenizer helps to tokenize Tweet Corpus into relevant tokens.\n",
        "\n",
        "The advantage of using TweetTokenizer() compared to regular word_tokenize is that, when processing tweets, we often come across emojis, hashtags that need to be handled differently."
      ],
      "metadata": {
        "id": "U4DchAWtPUfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['https://t.co/9z2J3P33Uc',\n",
        "               'laugh/cry',\n",
        "               'ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜±',\n",
        "               \"world's problems\",\n",
        "               \"@datageneral\",\n",
        "                \"It's interesting\",\n",
        "               \"don't spell my name right\",\n",
        "               'all-nighter']\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = []\n",
        "for sent in text:\n",
        "    print(tweet_tokenizer.tokenize(sent))\n",
        "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGMtAMXRRzX4",
        "outputId": "6fe74d19-d492-430c-f520-c3b489fe36a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://t.co/9z2J3P33Uc']\n",
            "['laugh', '/', 'cry']\n",
            "['ðŸ˜¬', 'ðŸ˜­', 'ðŸ˜“', 'ðŸ¤¢', 'ðŸ™„', 'ðŸ˜±']\n",
            "[\"world's\", 'problems']\n",
            "['@datageneral']\n",
            "[\"It's\", 'interesting']\n",
            "[\"don't\", 'spell', 'my', 'name', 'right']\n",
            "['all-nighter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Word Expression Tokenizer**\n",
        "\n",
        "A MWETokenizer takes a string which has already been divided into tokens and\n",
        "retokenizes it, merging multi-word expressions into single tokens, using a lexicon (dictionary) of MWEs."
      ],
      "metadata": {
        "id": "RdWyaRaqaT7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tk = MWETokenizer([('C', 'U'), ('Chandigarh', 'University')])\n",
        "tk.add_mwe(('who', 'are', 'you'))\n",
        "\n",
        "text = \"who are you at Chandigarh University\"\n",
        "\n",
        "text1 = tk.tokenize(text.split())\n",
        "   \n",
        "print(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRl2Keo-XSg1",
        "outputId": "c0ac63cb-fab1-4e62-fa4c-6a22ac631c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['who_are_you', 'at', 'Chandigarh_University']\n"
          ]
        }
      ]
    }
  ]
}