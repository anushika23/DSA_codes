{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Stop Words by Shubharthak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. \n",
    "\n",
    "We would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. You can find them in the nltk_data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sentence: This is a sample sentence, showing off the stop words filtration.\n",
      "Tokenized Sentence: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "After removing stop words: ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "print(f\"Sample Sentence: {example_sent}\")\n",
    "print(f\"Tokenized Sentence: {word_tokens}\")\n",
    "print(f\"After removing stop words: {filtered_sentence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating own stop words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sentence: This is a sample sentence, showing off the stop words filtration.\n",
      "Tokenized Sentence: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "After removing stop words: ['This', 'sample', 'sentence', ',', 'showing', 'the', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stopwords = {'hers', 'ourselves', 'is', 'she', 'her', 'in', 'if', 'a', 'my', 'off'}\n",
    "word_tokenized = word_tokenize(example_sent)\n",
    "stop_word_remover = [w for w in word_tokenized if not w in stopwords]\n",
    "print(f\"Sample Sentence: {example_sent}\")\n",
    "print(f\"Tokenized Sentence: {word_tokenized}\")\n",
    "print(f\"After removing stop words: {stop_word_remover}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
