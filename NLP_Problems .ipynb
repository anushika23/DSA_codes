{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9eZX23MxSIm"
   },
   "source": [
    "# Experiment 1 Tokenize the senetnce and paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgHMRmmRv6Tw",
    "outputId": "1275fd3b-42ae-406f-9a0d-45aa76120f3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anavj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "ZWwbAmUqwQM7"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize , sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "l1XiQ5oCwell"
   },
   "outputs": [],
   "source": [
    "text = \"Over the course of the tournament, many batting teams have gotten off to a terrific start in the powerplay before squandering it in the middle overs. Today, however, a tight opening spell from Wayne Parnell and Mohammed Siraj had Abhishek Sharma and Rahul Tripathi in check. However, as soon as they cut loose against Parnell with a 16-run over to get the run-rate from sub-3rpo to almost 7rpo, Michael Bracewell scalped them both in his very first over, rendering the powerplay a failure for the SRH innings.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPydXqtowwrk",
    "outputId": "30383acf-759b-4bfc-9b95-c196da4b47e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Over', 'the', 'course', 'of', 'the', 'tournament', ',', 'many', 'batting', 'teams', 'have', 'gotten', 'off', 'to', 'a', 'terrific', 'start', 'in', 'the', 'powerplay', 'before', 'squandering', 'it', 'in', 'the', 'middle', 'overs', '.', 'Today', ',', 'however', ',', 'a', 'tight', 'opening', 'spell', 'from', 'Wayne', 'Parnell', 'and', 'Mohammed', 'Siraj', 'had', 'Abhishek', 'Sharma', 'and', 'Rahul', 'Tripathi', 'in', 'check', '.', 'However', ',', 'as', 'soon', 'as', 'they', 'cut', 'loose', 'against', 'Parnell', 'with', 'a', '16-run', 'over', 'to', 'get', 'the', 'run-rate', 'from', 'sub-3rpo', 'to', 'almost', '7rpo', ',', 'Michael', 'Bracewell', 'scalped', 'them', 'both', 'in', 'his', 'very', 'first', 'over', ',', 'rendering', 'the', 'powerplay', 'a', 'failure', 'for', 'the', 'SRH', 'innings', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFcm3R8Fw66r",
    "outputId": "34b87eea-bbff-46b2-acbb-af75152cd24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Over the course of the tournament, many batting teams have gotten off to a terrific start in the powerplay before squandering it in the middle overs.', 'Today, however, a tight opening spell from Wayne Parnell and Mohammed Siraj had Abhishek Sharma and Rahul Tripathi in check.', 'However, as soon as they cut loose against Parnell with a 16-run over to get the run-rate from sub-3rpo to almost 7rpo, Michael Bracewell scalped them both in his very first over, rendering the powerplay a failure for the SRH innings.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbrUlb4QxZVi"
   },
   "source": [
    "# Experiment 2 Stemming lemminization and POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QG-Djk3Qm7a"
   },
   "source": [
    "## Port Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "mvtpo7PhxJQy"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "818-xneFQFtn"
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "8c-3pitrQM0_"
   },
   "outputs": [],
   "source": [
    "words = ['likes','sexy','hoty','pythonly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Py3ujO4JQSpS",
    "outputId": "b57a24f1-d8f1-4632-c5ee-4442173cc1ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes --> like\n",
      "sexy --> sexi\n",
      "hoty --> hoti\n",
      "pythonly --> pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "  print(f\"{w} --> {ps.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii-pTAEh0II8"
   },
   "source": [
    "## lemmanization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2TXE1ah0wPA",
    "outputId": "241d422b-b5be-48c8-dd28-5edfffe257fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\anavj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anavj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "d8FGvHEgQiPN"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "gJ-dHx3-0Qn1"
   },
   "outputs": [],
   "source": [
    "sentence = \"The bats are playing with their feet.\"\n",
    "words = [\"bats\", \"are\", \"feet\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "nymeNwoW0dT0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized sentence: ['The', 'bat', 'are', 'playing', 'with', 'their', 'foot', '.']\n",
      "Lemmatized words: ['bat', 'are', 'foot']\n"
     ]
    }
   ],
   "source": [
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Lemmatized sentence:\", lemmatized_sentence)\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO4hkybM05yU"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xWSoOYH0n5K",
    "outputId": "9b3233ba-dcb7-4a47-e2c0-4850365b06da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\anavj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "0sUp1Lme0-3K"
   },
   "outputs": [],
   "source": [
    "text = \" Michael Bracewell scalped them both in his very first over.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "3GzTdReM1KrV"
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DynNf271Si0",
    "outputId": "a73ad287-bba1-47ea-8fe0-c61a0b2fe59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Michael', 'NNP'), ('Bracewell', 'NNP'), ('scalped', 'VBD'), ('them', 'PRP'), ('both', 'DT'), ('in', 'IN'), ('his', 'PRP$'), ('very', 'RB'), ('first', 'RB'), ('over', 'IN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tags = pos_tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSYOFNHT2EQg"
   },
   "source": [
    "# Experiment 3 Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "ezNrzjDg1ZAo"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMGG4bDV2e1k",
    "outputId": "acf2619c-939b-4aaf-a538-3c67e6077d73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anavj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "cj6AIDRq2fv-"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.casefold() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZpt4-ij243D",
    "outputId": "c7a6e4a1-c611-46d7-b760-9d4c0f9c31d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before removing the stop words :- This is an example sentence to demonstrate stop word removal.\n",
      "Sentence after removing the stop words  :- example sentence demonstrate stop word removal .\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence to demonstrate stop word removal.\"\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(f\"Sentence before removing the stop words :- {text}\")\n",
    "print(f\"Sentence after removing the stop words  :- {filtered_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyMe7Reh3OYv"
   },
   "source": [
    "# Experiment 4 N gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KP9N-E3O28m6",
    "outputId": "a7e0ae92-fa89-41d7-9af1-8141c552bcaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anavj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the necessary NLTK data\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "wKixSKop3tts"
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    ngrams_list = list(ngrams(tokenized_text, n))\n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "FGkJu5kh3xf0"
   },
   "outputs": [],
   "source": [
    "text = \"Liam Livingstone six-hitting kept Kings in the game, just about, and when the last over began, he was on strike with 33 required - a six every ball. The win seemed to go out of the window when he swung and missed at the first ball from Ishant Sharma, but a no-ball for a high-full toss later in the over put Kings back in contention - at least mathematically - with 16 needed off the last three balls.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJPCUOgE4P9n",
    "outputId": "67479e42-ed42-4095-bb28-ae70c83da6f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the n-gram order: 1\n"
     ]
    }
   ],
   "source": [
    "ngram_order = int(input(\"Enter the n-gram order: \"))\n",
    "\n",
    "# imput 1 for unigram , 2 for bigram and 3 for trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "HvFOm9eu4T33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Liam',)\n",
      "('Livingstone',)\n",
      "('six-hitting',)\n",
      "('kept',)\n",
      "('Kings',)\n",
      "('in',)\n",
      "('the',)\n",
      "('game',)\n",
      "(',',)\n",
      "('just',)\n",
      "('about',)\n",
      "(',',)\n",
      "('and',)\n",
      "('when',)\n",
      "('the',)\n",
      "('last',)\n",
      "('over',)\n",
      "('began',)\n",
      "(',',)\n",
      "('he',)\n",
      "('was',)\n",
      "('on',)\n",
      "('strike',)\n",
      "('with',)\n",
      "('33',)\n",
      "('required',)\n",
      "('-',)\n",
      "('a',)\n",
      "('six',)\n",
      "('every',)\n",
      "('ball',)\n",
      "('.',)\n",
      "('The',)\n",
      "('win',)\n",
      "('seemed',)\n",
      "('to',)\n",
      "('go',)\n",
      "('out',)\n",
      "('of',)\n",
      "('the',)\n",
      "('window',)\n",
      "('when',)\n",
      "('he',)\n",
      "('swung',)\n",
      "('and',)\n",
      "('missed',)\n",
      "('at',)\n",
      "('the',)\n",
      "('first',)\n",
      "('ball',)\n",
      "('from',)\n",
      "('Ishant',)\n",
      "('Sharma',)\n",
      "(',',)\n",
      "('but',)\n",
      "('a',)\n",
      "('no-ball',)\n",
      "('for',)\n",
      "('a',)\n",
      "('high-full',)\n",
      "('toss',)\n",
      "('later',)\n",
      "('in',)\n",
      "('the',)\n",
      "('over',)\n",
      "('put',)\n",
      "('Kings',)\n",
      "('back',)\n",
      "('in',)\n",
      "('contention',)\n",
      "('-',)\n",
      "('at',)\n",
      "('least',)\n",
      "('mathematically',)\n",
      "('-',)\n",
      "('with',)\n",
      "('16',)\n",
      "('needed',)\n",
      "('off',)\n",
      "('the',)\n",
      "('last',)\n",
      "('three',)\n",
      "('balls',)\n",
      "('.',)\n"
     ]
    }
   ],
   "source": [
    "ngrams_result = generate_ngrams(text, ngram_order)\n",
    "for ngram in ngrams_result:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXrNcKWi4x92"
   },
   "source": [
    "# Experiment 5 Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTgeT9kl5WND"
   },
   "source": [
    "##  Using count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "x2NZ1k1R4XC0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "ZS9iQsAz5djZ"
   },
   "outputs": [],
   "source": [
    "def get_bag_of_words(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag_of_words = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return bag_of_words, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MoFUD0b5e-P",
    "outputId": "c44be973-80a3-45a8-98e9-ecac788b4cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of texts: 2\n",
      "Enter text 1: aj\n",
      "Enter text 2: alpha\n"
     ]
    }
   ],
   "source": [
    "num_texts = int(input(\"Enter the number of texts: \"))\n",
    "texts = []\n",
    "for i in range(num_texts):\n",
    "    text = input(f\"Enter text {i+1}: \")\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "2psh1BCx5hac"
   },
   "outputs": [],
   "source": [
    "bag_of_words, feature_names = get_bag_of_words(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_KabNdB6m26",
    "outputId": "07a746e4-0093-414e-ba01-00ded4b5f5d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag-of-words matrix:\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "\n",
      "Feature names:\n",
      "['aj' 'alpha']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBag-of-words matrix:\")\n",
    "print(bag_of_words.toarray())\n",
    "print(\"\\nFeature names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUcso6MZ8Rmb"
   },
   "source": [
    "## Self defined function(stop words + tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "-WCU4M2n8zcU"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "pzau_Pct60nO"
   },
   "outputs": [],
   "source": [
    "def vectorize(tokens):\n",
    "  vector=[]\n",
    "  for w in filtered_vocab:\n",
    "    vector.append(tokens.count(w))\n",
    "  return vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "6KjDqrhr8nkn"
   },
   "outputs": [],
   "source": [
    "def unique(sequence):\n",
    "  seen = set()\n",
    "  return [x for x in sequence if not(x in seen or seen.add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "YCSyan2S8xYc"
   },
   "outputs": [],
   "source": [
    "stopwords = ['to','is','a','the','are','an']\n",
    "special_char = [';',':','.','?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZAmROF89H7v",
    "outputId": "127d80c8-04a9-401b-bd41-0c8f6855f2e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the string 1 lamao\n",
      "Enter the string 2 hao\n"
     ]
    }
   ],
   "source": [
    "string1 = input(\"Enter the string 1 \")\n",
    "string2 = input(\"Enter the string 2 \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "sJzOO3aU9McT"
   },
   "outputs": [],
   "source": [
    "tokens1=string1.split()\n",
    "tokens2=string2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "A5TowTc89lkh"
   },
   "outputs": [],
   "source": [
    "vocab= unique(tokens1+tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "uBrCTDRk9yCS"
   },
   "outputs": [],
   "source": [
    "filtered_vocab=[]\n",
    "for w in vocab:\n",
    "  if w not in stopwords and w not in special_char:\n",
    "    filtered_vocab.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "a3c3Sius9_al"
   },
   "outputs": [],
   "source": [
    "vector1 = vectorize(tokens1)\n",
    "vector2 = vectorize(tokens2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEsIUVvu-IJS",
    "outputId": "39781020-db52-450b-c7a0-654484d67194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lamao', 'hao']\n",
      "[1, 0]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_vocab)\n",
    "print(vector1)\n",
    "print(vector2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20Y_XCFM-XOj"
   },
   "source": [
    "# EXP 6 - HMM MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\anavj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Download the Brown corpus\n",
    "nltk.download('brown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three sentences\n",
    "sentences = [\n",
    "    \"I love eating pizza.\",\n",
    "    \"will you eat pizza.\",\n",
    "    \"can you come home ?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged sentences:\n",
      "[('I', 'PRP'), ('love', 'VBP'), ('eating', 'VBG'), ('pizza', 'NN'), ('.', '.')]\n",
      "[('will', 'MD'), ('you', 'PRP'), ('eat', 'VB'), ('pizza', 'NNS'), ('.', '.')]\n",
      "[('can', 'MD'), ('you', 'PRP'), ('come', 'VB'), ('home', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS tag the sentences\n",
    "pos_tagged_sentences = [nltk.pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "print(\"POS tagged sentences:\")\n",
    "for pos_tagged_sentence in pos_tagged_sentences:\n",
    "    print(pos_tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a HMM tagger on the Brown corpus\n",
    "brown_train = brown.tagged_sents(categories='news')\n",
    "hmm_tagger = nltk.HiddenMarkovModelTagger.train(brown_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HMM tagged sentences:\n",
      "[('I', 'PPSS'), ('love', 'VB'), ('eating', 'AT'), ('pizza', 'NN'), ('.', '.')]\n",
      "[('will', '``'), ('you', 'PPSS'), ('eat', 'VB'), ('pizza', 'NN'), ('.', '.')]\n",
      "[('can', '``'), ('you', 'PPSS'), ('come', 'VB'), ('home', 'NR'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "# HMM tag the sentences\n",
    "hmm_tagged_sentences = [hmm_tagger.tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "print(\"\\nHMM tagged sentences:\")\n",
    "for hmm_tagged_sentence in hmm_tagged_sentences:\n",
    "    print(hmm_tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HMM model:\n",
      "<HiddenMarkovModelTagger 218 states and 14394 output symbols>\n"
     ]
    }
   ],
   "source": [
    "# Print the HMM model\n",
    "print(\"\\nHMM model:\")\n",
    "print(hmm_tagger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emission probabilities:\n",
      "{'I': {'PRP': 1.0}, 'love': {'PRP': 0.0}, 'eating': {'PRP': 0.0}, 'pizza': {'PRP': 0.0}, '.': {'PRP': 0.0}, 'will': {'PRP': 0.0}, 'you': {'PRP': 1.0}, 'eat': {'PRP': 0.0}, 'can': {'PRP': 0.0}, 'come': {'PRP': 0.0}, 'home': {'PRP': 0.0}, '?': {'PRP': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# Calculate emission probabilities\n",
    "emission_probabilities = {}\n",
    "for sentence, pos_tagged_sentence in zip(sentences, pos_tagged_sentences):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        emission_count = sum([1 for _, t in pos_tagged_sentence if t == pos and _ == token])\n",
    "        total_pos_count = sum([1 for _, t in pos_tagged_sentence if t == pos])\n",
    "        emission_probabilities.setdefault(token, {})[pos] = emission_count / total_pos_count\n",
    "print(\"\\nEmission probabilities:\")\n",
    "print(emission_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transition probabilities:\n",
      "{('PRP', 'VBP'): {'PRP': 1.0}, ('VBP', 'VBG'): {'VBP': 1.0}, ('VBG', 'NN'): {'VBG': 1.0}, ('NN', '.'): {'NN': 1.0}, ('MD', 'PRP'): {'MD': 1.0}, ('PRP', 'VB'): {'PRP': 1.0}, ('VB', 'NNS'): {'VB': 1.0}, ('NNS', '.'): {'NNS': 1.0}, ('VB', 'NN'): {'VB': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# Calculate transition probabilities\n",
    "transition_probabilities = {}\n",
    "for sentence, pos_tagged_sentence in zip(sentences, pos_tagged_sentences):\n",
    "    tags = [tag for _, tag in pos_tagged_sentence]\n",
    "    for i in range(len(tags) - 1):\n",
    "        transition_count = sum([1 for j in range(len(tags) - 1) if tags[j] == tags[i] and tags[j+1] == tags[i+1]])\n",
    "        total_tag_count = sum([1 for j in range(len(tags) - 1) if tags[j] == tags[i]])\n",
    "        transition_probabilities.setdefault((tags[i], tags[i+1]), {})[tags[i]] = transition_count / total_tag_count\n",
    "print(\"\\nTransition probabilities:\")\n",
    "print(transition_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP - 7 POS TAG USING VITERBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRON'), ('LOVE', 'PRON'), ('READING', 'PRON')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Load your training data\n",
    "train_data = [\n",
    "    [(\"I\", \"PRON\"), (\"love\", \"VERB\"), (\"eating\", \"VERB\"), (\"pizza\", \"NOUN\")],\n",
    "    [(\"She\", \"PRON\"), (\"enjoys\", \"VERB\"), (\"reading\", \"VERB\"), (\"books\", \"NOUN\")]\n",
    "]\n",
    "\n",
    "# Create an instance of the HMM trainer\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "# Train the HMM tagger on your training data\n",
    "tagger = trainer.train_supervised(train_data)\n",
    "\n",
    "sentence = \"I LOVE READING\"\n",
    "tagged_sentence = tagger.tag(nltk.word_tokenize(sentence))\n",
    "print(tagged_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optional training datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# # giving u guys extra train data , don't use same training data as everyone \n",
    "\n",
    "# [\n",
    "#     [(\"They\", \"PRON\"), (\"play\", \"VERB\"), (\"football\", \"NOUN\")],\n",
    "#     [(\"We\", \"PRON\"), (\"watch\", \"VERB\"), (\"movies\", \"NOUN\")]\n",
    "# ]\n",
    "\n",
    "\n",
    "# [\n",
    "#     [(\"He\", \"PRON\"), (\"likes\", \"VERB\"), (\"swimming\", \"VERB\")],\n",
    "#     [(\"She\", \"PRON\"), (\"dances\", \"VERB\"), (\"beautifully\", \"ADV\")]\n",
    "# ]\n",
    "\n",
    "\n",
    "# [\n",
    "#     [(\"The\", \"DET\"), (\"cat\", \"NOUN\"), (\"is\", \"VERB\"), (\"sleeping\", \"VERB\")],\n",
    "#     [(\"A\", \"DET\"), (\"dog\", \"NOUN\"), (\"is\", \"VERB\"), (\"barking\", \"VERB\")]\n",
    "# ]\n",
    "\n",
    "\n",
    "# [\n",
    "#     [(\"I\", \"PRON\"), (\"like\", \"VERB\"), (\"playing\", \"VERB\"), (\"tennis\", \"NOUN\")],\n",
    "#     [(\"He\", \"PRON\"), (\"enjoys\", \"VERB\"), (\"listening\", \"VERB\"), (\"to\", \"PRT\"), (\"music\", \"NOUN\")]\n",
    "# ]\n",
    "\n",
    "\n",
    "# [\n",
    "#     [(\"She\", \"PRON\"), (\"runs\", \"VERB\"), (\"fast\", \"ADV\")],\n",
    "#     [(\"He\", \"PRON\"), (\"reads\", \"VERB\"), (\"books\", \"NOUN\"), (\"regularly\", \"ADV\")]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 8 - POS TAG USING SPACY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#use only in collab \n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"My name is anav\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "svg = displacy.render(doc, style=\"dep\", jupyter=False)\n",
    "\n",
    "with open('dependency_parse.svg', 'w', encoding='utf-8') as f:\n",
    "    f.write(svg)\n",
    "#see this code is little tricky , it downloads u a web page of output with a correct diagram \n",
    "# so if it doesnt work in jupyter use collab , when u run , wait a little it will create \n",
    "# a file named dependency.svg in your file section in collab at the left side of code \n",
    "# blocks just download it and open in your pc ... if facing problem just ask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 9 - CHUNKING WITH NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Anav', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"My name is Anav\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT big/JJ red/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# define a grammar for noun phrases\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+} # chunk sequences of determiner, adjective, and noun tags\n",
    "\"\"\"\n",
    "\n",
    "# create a chunk parser using the grammar\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "# tokenize a sentence into POS tags\n",
    "sentence = [('the', 'DT'), ('big', 'JJ'), ('red', 'JJ'), ('dog', 'NN'),\n",
    "            ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN')]\n",
    "\n",
    "# parse the sentence to identify noun phrases\n",
    "tree = chunk_parser.parse(sentence)\n",
    "\n",
    "# print the resulting parse tree\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT cat/NN)\n",
      "  (PP in/IN (NP the/DT hat/NN))\n",
      "  sat/VBD\n",
      "  (PP on/IN (NP the/DT mat/NN))\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# define a grammar for chunking\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+} # noun phrase chunking\n",
    "  PP: {<IN><NP>} # prepositional phrase chunking\n",
    "\"\"\"\n",
    "\n",
    "# create a chunk parser using the grammar\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "# tokenize a sentence into POS tags\n",
    "sentence = nltk.pos_tag(nltk.word_tokenize(\"The cat in the hat sat on the mat.\"))\n",
    "\n",
    "# parse the sentence to identify noun and prepositional phrases\n",
    "tree = chunk_parser.parse(sentence)\n",
    "\n",
    "# print the resulting parse tree\n",
    "print(tree)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
